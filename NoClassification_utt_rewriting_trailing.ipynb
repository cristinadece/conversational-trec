{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting strategy, no classification is used, no memory but trailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# from conversationalir.uttclassification.conversation_features import utterance_cosine_similarity_first, utterance_cosine_similarity_previous,is_next_sentence_to_first_neural, is_next_sentence_to_previous_neural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"../CAST_2020/\"\n",
    "\n",
    "utt_file = path+\"data/2020_raw.tsv\"\n",
    "test_df = pd.read_csv(utt_file, delimiter=\"\\t\")\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def create_doc(utt):\n",
    "    return nlp(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person_prons = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"them\", \"his\", \"her\", \"its\", \"their\"]\n",
    "\n",
    "def _rewrite_utt(doc, first_topic=\"\", previous_topic=\"\", context_list=None, trailing=False):\n",
    "        new_utt = \"\"\n",
    "        for token in doc:\n",
    "            if (token.tag_ == \"PRP\" or token.tag_ == \"PRP$\") \\\n",
    "                    and token.text in third_person_prons:\n",
    "                if previous_topic != \"\":\n",
    "                    new_utt += previous_topic + \" \"\n",
    "                if first_topic != \"\":\n",
    "                    new_utt += first_topic + \" \"\n",
    "            else:\n",
    "                new_utt += token.text + \" \"\n",
    "\n",
    "        # TRAILING THE TOPIC\n",
    "        if trailing:\n",
    "            if previous_topic.lower() not in new_utt.lower():\n",
    "                new_utt += previous_topic + \" \"\n",
    "            if first_topic.lower() not in new_utt.lower():\n",
    "                new_utt += first_topic\n",
    "\n",
    "        return new_utt\n",
    "    \n",
    "def _rewrite_utt_new(utt, first_topic=\"\", previous_topic=\"\", context_list=None, trailing=False):\n",
    "        new_utt = utt\n",
    "        #print(\"utt: \"+utt)\n",
    "        #print(\"first topic: \"+first_topic)\n",
    "        #print(\"prev topic: \"+previous_topic)\n",
    "        if trailing:\n",
    "            if previous_topic.lower() not in new_utt.lower():\n",
    "                new_utt += \" \"+previous_topic\n",
    "            if first_topic.lower() not in new_utt.lower():\n",
    "                new_utt += \" \" + first_topic\n",
    "\n",
    "        return new_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_cue_topic(doc):\n",
    "    cue_phrases = [\"tell me more about\", \"tell me about\", \"what about\", \"how about\"]\n",
    "    third_person_prons = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"them\", \"his\", \"her\", \"its\", \"their\"]\n",
    "    current_topic = \"\"\n",
    "    pron = False\n",
    "    for cue in cue_phrases:\n",
    "        if cue in str(doc).lower():\n",
    "            # check if pron:\n",
    "            for token in doc:\n",
    "                if (token.tag_ == \"PRP\" or token.tag_ == \"PRP$\") and \\\n",
    "                        token.text in third_person_prons:\n",
    "                    pron = True\n",
    "            if not pron:\n",
    "                current_topic = str(doc).lower().replace(cue, \"\").replace(\".\", \"\").replace(\"?\", \"\")\n",
    "    return current_topic\n",
    "\n",
    "\n",
    "def _find_topic_new(doc):\n",
    "    #pos_list = [\"nsubj\", \"dobj\", \"pobj\"]\n",
    "    topic = \"\"\n",
    "    sw_list = [\"the\", \"a\", \"an\", \"how\", \"many\", \"much\", \"when\", \"where\", \"who\", \"what\", \"why\"]\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ not in [\"PRON\", \"AUX\", \"ADP\", \"PUNCT\"]:\n",
    "            if token.text.lower() not in sw_list:\n",
    "                topic += \" \" + token.text \n",
    "                \n",
    "    # NO TOPIC - trick for \"Describe Uranus.\"\n",
    "    if topic == \"\":\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"PUNCT\"]:\n",
    "                if token.text.lower() not in sw_list:\n",
    "                    topic += token.text + \" \"\n",
    "\n",
    "    return topic\n",
    "\n",
    "def _find_topic(doc):\n",
    "    #pos_list = [\"nsubj\", \"dobj\", \"pobj\"]\n",
    "    topic = \"\"\n",
    "    \n",
    "    # NEW FEATURE\n",
    "    # check if it's a cue topic first\n",
    "    cue_topic = _find_cue_topic(doc)\n",
    "    if cue_topic!=\"\":\n",
    "        return cue_topic\n",
    "    \n",
    "    # GET the first topic\n",
    "    for chunk in doc.noun_chunks:\n",
    "        #if chunk.root.dep_ in pos_list:\n",
    "        if chunk.root.pos_ != \"PRON\":\n",
    "            topic += \" \" + chunk.text\n",
    "\n",
    "    # NO FIRST TOPIC - trick for \"Describe Uranus.\"\n",
    "    if topic == \"\":\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"VERB\", \"PUNCT\"]:\n",
    "                topic += token.text + \" \"\n",
    "\n",
    "    return topic\n",
    "\n",
    "\n",
    "def _find_topic_all(doc):\n",
    "    pos_list = [\"nsubj\", \"dobj\", \"pobj\"]\n",
    "    topic = \"\"\n",
    "    \n",
    "    # NEW FEATURE\n",
    "    # check if it's a cue topic first\n",
    "    cue_topic = _find_cue_topic(doc)\n",
    "    if cue_topic!=\"\":\n",
    "        return cue_topic\n",
    "    \n",
    "    # GET the topic\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.pos_ != \"PRON\":\n",
    "            topic += \" \" + chunk.text\n",
    "\n",
    "    # NO TOPIC - trick for \"Describe Uranus.\"\n",
    "    if topic == \"\":\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"VERB\", \"PUNCT\"]:\n",
    "                topic += token.text + \" \"\n",
    "\n",
    "    return topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_doc_tmp = create_doc(\"How about snowboarding ?\")\n",
    "topic_tmp = _find_cue_topic(utt_doc_tmp)\n",
    "\n",
    "print(topic_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 4 (CAST 2020): trailing without memory. It always propagates first/current topic (current only if there was a topic shift) and the previous topic. Topics are noun chunks + verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_utt_dict = {}\n",
    "\n",
    "current_topic = {}\n",
    "utterance_topic = {}\n",
    "\n",
    "for i in range(0,len(test_df)):\n",
    "    utt_id = test_df['utt_id'][i]\n",
    "    prev_utt_id = utt_id.split(\"_\")[0]+\"_\"+str(int(utt_id.split(\"_\")[1])-1)\n",
    "    \n",
    "    if (utt_id.split(\"_\")[1]==str(1)): # first utterace\n",
    "        first_utt = test_df['utt'][i]\n",
    "        first_utt_doc = create_doc(first_utt)\n",
    "        first_topic = _find_topic(first_utt_doc)\n",
    "        current_topic [utt_id] = first_topic #here first = current topic\n",
    "        utterance_topic [utt_id] = first_topic\n",
    "        #propagation\n",
    "        enriched_utt_dict[utt_id] = first_utt #always SE, so no propagation\n",
    "       \n",
    "    else:\n",
    "       \n",
    "        utt = test_df['utt'][i]\n",
    "        utt_doc = create_doc(utt)\n",
    "        \n",
    "        #take the utterance topic\n",
    "        utt_topic = _find_topic(utt_doc)\n",
    "        utterance_topic [utt_id] = utt_topic\n",
    "        \n",
    "        #see if there is also a topic shift\n",
    "        curr_topic = _find_cue_topic (utt_doc)\n",
    "        if curr_topic != \"\":\n",
    "            current_topic [utt_id] = curr_topic #there is a topic shift, so the current topic is updated\n",
    "        else:\n",
    "            current_topic [utt_id] = current_topic[prev_utt_id] #no topic shift, so the current topic is taken from the previous one        \n",
    "        \n",
    "        \n",
    "        #propagation of current and previous topics\n",
    "        prev_topic_to_propagate = utterance_topic [prev_utt_id]\n",
    "        curr_topic_to_propagate = current_topic [prev_utt_id]\n",
    "        \n",
    "        new_utt = _rewrite_utt_new(utt, first_topic = curr_topic_to_propagate, \n",
    "                                   previous_topic = prev_topic_to_propagate, context_list=None, trailing=True)\n",
    "        enriched_utt_dict[utt_id] = new_utt     \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results -> this has been submitted to CAsT 2020 as run 4 to be more different from all context (chosen as run 2)\n",
    "with open(path+\"data/CAST2020_rewritten_utterances_curr_prev_verbs.tsv\", \"w\") as f:\n",
    "    for k,v in enriched_utt_dict.items():\n",
    "        f.write(\"{}\\t{}\\n\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding passage of previous utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original passages\n",
    "passages_file = path+\"data/2020_automatic_eval_withCleanedPassage.tsv\"\n",
    "passages_df = pd.read_csv(passages_file, delimiter=\"\\t\", header = None)\n",
    "\n",
    "print(passages_df.head())\n",
    "\n",
    "passages = dict(passages_df.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path+\"data/CAST2020_rewritten_utterances_curr_prev_verbs_withCleanedPassage.tsv\", \"w\") as f:\n",
    "    for k,v in enriched_utt_dict.items():\n",
    "        utt_id = k\n",
    "        if (utt_id.split(\"_\")[1]==str(1)): # first utterace\n",
    "            f.write(\"{}\\t{}\\n\".format(k,v))\n",
    "        else:   \n",
    "            utt = v\n",
    "            prev_utt_id = utt_id.split(\"_\")[0]+\"_\"+str(int(utt_id.split(\"_\")[1])-1)\n",
    "            passage = passages[prev_utt_id]\n",
    "            new_utt = utt + \" \" +passage\n",
    "            f.write(\"{}\\t{}\\n\".format(utt_id,new_utt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always propagate *with memory: topics are extracted from previous enriched utterance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_utt_dict = {}\n",
    "\n",
    "for i in range(0,len(test_df)):\n",
    "    utt_id = test_df['utt_id'][i]\n",
    "    prev_utt_id = utt_id.split(\"_\")[0]+\"_\"+str(int(utt_id.split(\"_\")[1])-1)\n",
    "    \n",
    "    if (utt_id.split(\"_\")[1]==str(1)): # first utterace\n",
    "        first_utt = test_df['utt'][i]\n",
    "        first_utt_doc = create_doc(first_utt)\n",
    "        first_topic = _find_topic_new(first_utt_doc)\n",
    "        \n",
    "        #propagation\n",
    "        enriched_utt_dict[utt_id] = first_utt # always SE\n",
    "\n",
    "    else:\n",
    "        #label = test_df['final_label'][i]\n",
    "        current_utt = test_df['utt'][i]\n",
    "        current_utt_doc = create_doc(current_utt)\n",
    "        \n",
    "        # get the topic from the enriched previous utterance\n",
    "        prev_utt_doc = create_doc(enriched_utt_dict[prev_utt_id]) \n",
    "        prev_topic = _find_topic_new(prev_utt_doc)\n",
    "        \n",
    "        new_utt = _rewrite_utt_new(current_utt, first_topic = \"\", previous_topic=prev_topic, context_list=None, trailing=True)\n",
    "        enriched_utt_dict[utt_id] = new_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results\n",
    "with open(path+\"data/CAST2020_rewritten_utterances_memory.tsv\", \"w\") as f:\n",
    "    for k,v in enriched_utt_dict.items():\n",
    "        f.write(\"{}\\t{}\\n\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original passages\n",
    "passages_file = path+\"data/2020_automatic_eval_withCleanedPassage.tsv\"\n",
    "passages_df = pd.read_csv(passages_file, delimiter=\"\\t\", header = None)\n",
    "\n",
    "print(passages_df.head())\n",
    "\n",
    "passages = dict(passages_df.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"data/CAST2020_rewritten_utterances_memory_withCleanedPassage.tsv\", \"w\") as f:\n",
    "    for k,v in enriched_utt_dict.items():\n",
    "        utt_id = k\n",
    "        if (utt_id.split(\"_\")[1]==str(1)): # first utterace\n",
    "            f.write(\"{}\\t{}\\n\".format(k,v))\n",
    "        else:   \n",
    "            utt = v\n",
    "            prev_utt_id = utt_id.split(\"_\")[0]+\"_\"+str(int(utt_id.split(\"_\")[1])-1)\n",
    "            passage = passages[prev_utt_id]\n",
    "            new_utt = utt + \" \" +passage\n",
    "            f.write(\"{}\\t{}\\n\".format(utt_id,new_utt))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
